behaviors:
  MoveToTarget:
    trainer_type: ppo
    time_horizon: 256 # Typical range: 32 - 2048
    max_steps: 5e5 # Typical range: 5e5 - 1e7
    hyperparameters:
      learning_rate: 1e-5 # Typical range: 1e-5 - 1e-3
      beta: 1e-3
      batch_size: 512 # Typical range: (Continuous - PPO): 512 - 5120; (Continuous - SAC): 128 - 1024; (Discrete, PPO & SAC): 32 - 512.
      buffer_size: 10000 # Typical range: PPO: 2048 - 409600; SAC: 50000 - 1000000
      num_epoch: 3 # Typical range: 3 - 10, Number of passes to make through the experience buffer when performing gradient descent optimization.The larger the batch_size, the larger it is acceptable to make this. Decreasing this will ensure more stable updates, at the cost of slower learning.
      lambd: 0.95 #Typical range: 0.9-0.95, higher for high variance
      epsilon: 0.1
    network_settings:
      num_layers: 2 # Typical range: 1 - 3
      hidden_units: 256 # Typical range: 32 - 512
    reward_signals:
      extrinsic:
        gamma: 0.85
        strength: 1.0
      #curiosity:
        #gamma: 0.99
        #strength: 0.02
        #network_settings:
          #hidden_units: 256
        #learning_rate: 0.0003
    summary_freq: 3000
    threaded: true